{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Azure Machine Learning Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "from azureml.core import Run, Experiment, Datastore\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n",
    "\n",
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "print(\"Blobstore's name: {}\".format(def_blob_store.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Azure Machine Learning Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "aml_compute_target = \"cpu-cluster\"\n",
    "try:\n",
    "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
    "    print(\"Found existing compute target: {}\".format(aml_compute_target))\n",
    "except:\n",
    "    print(\"Creating new compute target: {}\".format(aml_compute_target))\n",
    "    \n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\",\n",
    "                                                                min_nodes = 1, \n",
    "                                                                max_nodes = 4)    \n",
    "    aml_compute = ComputeTarget.create(ws, aml_compute_target, provisioning_config)\n",
    "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Configuration for Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "\n",
    "# create a new runconfig object\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "# enable Docker \n",
    "run_config.environment.docker.enabled = True\n",
    "\n",
    "# set Docker base image to the default CPU-based image\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "\n",
    "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# specify CondaDependencies obj\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['scikit-learn'],\n",
    "                                                                            pip_packages=['azure-cognitiveservices-vision-customvision',\n",
    "                                                                                          'azure-cognitiveservices-search-imagesearch'],\n",
    "                                                                            pin_sdk_version=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Publish Pipeline For Training\n",
    "\n",
    "Build a pipeline to train on comics images, publish it and add a schedule to run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing pipeline parameters, inputs and outputs\n",
    "\n",
    "These will be pipeline parameters which can be override while submitting a new run from the pipeline. To keep it simple, we will provide one `tag_name` and an AzureBlob path `image_folder` where scrapper will download the images based on the `search_term` and save them prefixed with `tag_name`. Our training step will then read the files which are prefixed with the `tag_name` in the `image_folder` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineParameter, PipelineData\n",
    "\n",
    "image_tag = PipelineParameter(\"image_tag\", default_value=\"Dog\")\n",
    "search_term = PipelineParameter(\"search_term\", default_value=\"Adult dog cartoon\")\n",
    "image_folder = PipelineData(\"comics_images\",datastore=def_blob_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the image scrapper step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "scrapperStep = PythonScriptStep(\n",
    "    name=\"Image_Scrapper_Step\",\n",
    "    script_name=\"image_scrapper.py\",\n",
    "    arguments =[\"--scrapper_output_folder\", image_folder, \"--scrapper_image_tag\", image_tag, \n",
    "                \"--scrapper_search_term\", search_term],\n",
    "    outputs = [image_folder],\n",
    "    compute_target=aml_compute_target, \n",
    "    source_directory='./image_scrapper_step',\n",
    "    runconfig=run_config,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"ScrapperStep created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainStep = PythonScriptStep(\n",
    "    name=\"Training_Step\",\n",
    "    script_name=\"train.py\",\n",
    "    arguments =[\"--train_image_tag\", image_tag, \"--train_image_folder\", image_folder],\n",
    "    inputs = [image_folder],\n",
    "    compute_target=aml_compute_target, \n",
    "    source_directory='./train_step',\n",
    "    runconfig=run_config,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"TrainStep created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline1 = Pipeline(workspace=ws, steps=[trainStep])\n",
    "print (\"Pipeline is built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1.validate()\n",
    "print(\"Pipeline is validated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run1 = Experiment(ws, 'comic_reader_training').submit(pipeline1, regenerate_outputs=False)\n",
    "print(\"Pipeline is submitted for execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(pipeline_run1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "pipeline_name = \"comics_training_pipeline\"\n",
    "print(pipeline_name)\n",
    "\n",
    "published_pipeline1 = pipeline1.publish(\n",
    "    name=pipeline_name, \n",
    "    description=pipeline_name)\n",
    "print(\"Newly published pipeline id: {}\".format(published_pipeline1.id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get published pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PublishedPipeline\n",
    "\n",
    "pipeline_id = published_pipeline1.id # use your published pipeline id\n",
    "published_pipeline = PublishedPipeline.get(ws, pipeline_id)\n",
    "published_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run published pipeline using REST Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can perform HTTP POST on URL https://westus2.aether.ms/api/v1.0/subscriptions/b8c23406-f9b5-4ccb-8a65-a8cb5dcd6a5a/resourceGroups/pudixit-rg/providers/Microsoft.MachineLearningServices/workspaces/pudixit-ws/PipelineRuns/PipelineSubmit/4f0a5618-32c1-45b0-8b07-aa5fb4d2760e to trigger this pipeline\n",
      "4b9bb16b-2656-44f2-a931-858ef35bded4\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "import requests\n",
    "\n",
    "auth = InteractiveLoginAuthentication()\n",
    "aad_token = auth.get_authentication_header()\n",
    "\n",
    "rest_endpoint1 = published_pipeline.endpoint\n",
    "\n",
    "print(\"You can perform HTTP POST on URL {} to trigger this pipeline\".format(rest_endpoint1))\n",
    "\n",
    "# specify the param when running the pipeline\n",
    "response = requests.post(rest_endpoint1, \n",
    "                         headers=aad_token, \n",
    "                         json={\"ExperimentName\": \"Comics_Reader_Training\",\n",
    "                               \"RunSource\": \"SDK\",\n",
    "                               \"ParameterAssignments\": {\"image_tag\": \"sad_monkey_in_cage\", \"search_term\": \"sad monkey in a cage\"}})\n",
    "run_id = response.json()[\"Id\"]\n",
    "\n",
    "print(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "422ba17499224bdb900e8e831dbd96ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.pipeline.core import PipelineRun\n",
    "\n",
    "exp = ws.experiments['Comics_Reader_Training']\n",
    "published_pipeline_run = PipelineRun(exp, run_id)\n",
    "RunDetails(published_pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedule your published pipeline\n",
    "\n",
    "Schedule operations require id of a published pipeline. You can get all published pipelines and do Schedule operations on them, or if you already know the id of the published pipeline, you can use it directly as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\n",
    "\n",
    "recurrence = ScheduleRecurrence(frequency=\"Day\", interval=2, hours=[22], minutes=[30]) # Runs every other day at 10:30pm\n",
    "\n",
    "schedule = Schedule.create(workspace=ws, name=\"Comic_Reader_Schedule\",\n",
    "                           pipeline_id=pipeline_id, \n",
    "                           experiment_name='Comics_Reader_Training',\n",
    "                           recurrence=recurrence,\n",
    "                           wait_for_provisioning=True,\n",
    "                           description=\"Comics_Reader_Training\",\n",
    "                           pipeline_parameters={\"image_tag\": \"Man_with_a_hat\", \"search_term\": \"Man with a hat cartoon\"})\n",
    "\n",
    "# You may want to make sure that the schedule is provisioned properly\n",
    "# before making any further changes to the schedule\n",
    "\n",
    "print(\"Created schedule with id: {}\".format(schedule.id))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
